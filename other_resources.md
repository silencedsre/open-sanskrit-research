## Helpful Resources

**TENER: Adapting Transformer Encoder for Named Entity Recognition** \
https://arxiv.org/abs/1911.04474


**6A Lexicon-Based Graph Neural Network for Chinese NER (Lattice-GNN)** \
https://aclanthology.org/D19-1096/

**FLAT: Chinese NER Using Flat-Lattice Transformer** \
https://arxiv.org/abs/2004.11795


**Attention Is All You Need (Transformer):** Foundational paper introducing Transformers \
https://arxiv.org/abs/1706.03762

**ByT5: Towards a token-free future with pre-trained byte-to-byte models** \
https://arxiv.org/abs/2105.13626

**Byte Latent Transformer: Patches Scale Better Than Tokens** \
https://arxiv.org/pdf/2412.09871