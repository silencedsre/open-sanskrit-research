{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac759548-8bd6-4345-afd0-0e8e05ee1b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training subset: 20000 samples\n",
      "Validation subset: 200 samples\n",
      "Max sequence length: 256\n",
      "Effective batch size: 32\n",
      "Loading Itihasa dataset from local parquet files...\n",
      "\n",
      "Dataset structure:\n",
      "Train samples: 75162\n",
      "Validation samples: 6149\n",
      "Test samples: 11722\n",
      "\n",
      "================================================================================\n",
      "Sample from dataset:\n",
      "================================================================================\n",
      "\n",
      "Full sample structure: dict_keys(['translation'])\n",
      "\n",
      "Sample data:\n",
      "{'translation': {'en': 'The ascetic VƒÅlmƒ´ki asked NƒÅrada, the best of sages and foremost of those conversant with words, ever engaged in austerities and Vedic studies.', 'sn': '‡•ê ‡§§‡§™‡§É ‡§∏‡•ç‡§µ‡§æ‡§ß‡•ç‡§Ø‡§æ‡§Ø‡§®‡§ø‡§∞‡§§‡§Ç ‡§§‡§™‡§∏‡•ç‡§µ‡•Ä ‡§µ‡§æ‡§ó‡•ç‡§µ‡§ø‡§¶‡§æ‡§Ç ‡§µ‡§∞‡§Æ‡•ç‡•§ ‡§®‡§æ‡§∞‡§¶‡§Ç ‡§™‡§∞‡§ø‡§™‡§™‡•ç‡§∞‡§ö‡•ç‡§õ ‡§µ‡§æ‡§≤‡•ç‡§Æ‡•Ä‡§ï‡§ø‡§∞‡•ç‡§Æ‡•Å‡§®‡§ø‡§™‡•Å‡§ô‡•ç‡§ó‡§µ‡§Æ‡•ç‡••'}}\n",
      "\n",
      "Creating subsets...\n",
      "Train subset: 20000 samples\n",
      "Validation subset: 200 samples\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded. Vocab size: 68096\n",
      "Loading base model...\n",
      "‚ö†Ô∏è Note: Loading Sarvam-1 (2B parameters) - this may take a few minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cae20d802bc4fcaa3093bb69651ee29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Parameters: 2,525,087,744\n",
      "Configuring LoRA...\n",
      "trainable params: 6,422,528 || all params: 2,531,510,272 || trainable%: 0.2537\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffd878707d74918bc429138b0f1203a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing training data:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9687dacd6224407f97f76c642f5e76e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation data:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete\n",
      "Tokenized train samples: 20000\n",
      "Tokenized val samples: 200\n",
      "\n",
      "================================================================================\n",
      "DEBUGGING TOKENIZED DATA\n",
      "================================================================================\n",
      "\n",
      "--- Example 0 ---\n",
      "Sequence length: 256\n",
      "Actual tokens (no padding): 112\n",
      "Padding tokens: 144\n",
      "Non-masked tokens (trainable): 112\n",
      "Masked tokens: 144\n",
      "Percentage trainable: 43.8%\n",
      "\n",
      "--- Example 1 ---\n",
      "Sequence length: 256\n",
      "Actual tokens (no padding): 97\n",
      "Padding tokens: 159\n",
      "Non-masked tokens (trainable): 97\n",
      "Masked tokens: 159\n",
      "Percentage trainable: 37.9%\n",
      "\n",
      "--- Example 2 ---\n",
      "Sequence length: 256\n",
      "Actual tokens (no padding): 95\n",
      "Padding tokens: 161\n",
      "Non-masked tokens (trainable): 95\n",
      "Masked tokens: 161\n",
      "Percentage trainable: 37.1%\n",
      "================================================================================\n",
      "üìù Training Configuration:\n",
      "  - Model: sarvamai/sarvam-1\n",
      "  - Max sequence length: 256\n",
      "  - Batch size: 4\n",
      "  - Gradient accumulation: 8\n",
      "  - Effective batch size: 32\n",
      "  - Learning rate: 5e-06\n",
      "  - Total epochs: 1\n",
      "Trainer initialized successfully!\n",
      "\n",
      "================================================================================\n",
      "üöÄ Starting training...\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 4:30:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.014800</td>\n",
       "      <td>2.909653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.638900</td>\n",
       "      <td>2.585361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.521800</td>\n",
       "      <td>2.515157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.480700</td>\n",
       "      <td>2.484417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.463100</td>\n",
       "      <td>2.469894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.462200</td>\n",
       "      <td>2.464713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ Training complete!\n",
      "================================================================================\n",
      "\n",
      "Saving model to ./sarvam1-itihasa-lora...\n",
      "‚úÖ Model saved successfully!\n",
      "‚úì Location: ./sarvam1-itihasa-lora\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # LoRA Finetuning: Sarvam-1 on Itihasa Dataset\n",
    "# Optimized for Apple Silicon (M4 Mac)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Check for MPS availability\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Configuration\n",
    "\n",
    "# %%\n",
    "# Model configuration\n",
    "MODEL_NAME = \"sarvamai/sarvam-1\"\n",
    "OUTPUT_DIR = \"./sarvam1-itihasa-lora\"\n",
    "\n",
    "# Dataset subset configuration\n",
    "TRAIN_SUBSET_SIZE = 20000  # Use first 20000 samples for training\n",
    "VAL_SUBSET_SIZE = 200      # Use first 500 samples for validation\n",
    "\n",
    "# Training configuration\n",
    "MAX_SEQ_LENGTH = 256       # Sarvam-1 can handle longer sequences\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 4           # Reduced for 2B model\n",
    "GRADIENT_ACCUM_STEPS = 8   # Increased to compensate\n",
    "LEARNING_RATE = 5e-6       # Lower LR for larger model\n",
    "\n",
    "print(f\"Training subset: {TRAIN_SUBSET_SIZE} samples\")\n",
    "print(f\"Validation subset: {VAL_SUBSET_SIZE} samples\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM_STEPS}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Load Dataset from Local Parquet Files\n",
    "\n",
    "# %%\n",
    "# Load dataset from local parquet files\n",
    "print(\"Loading Itihasa dataset from local parquet files...\")\n",
    "dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\n",
    "        \"train\": \"ithasa/train/0000.parquet\",\n",
    "        \"validation\": \"ithasa/val/0000.parquet\",\n",
    "        \"test\": \"ithasa/test/0000.parquet\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Explore Dataset\n",
    "\n",
    "# %%\n",
    "# Show sample data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample from dataset:\")\n",
    "print(\"=\"*80)\n",
    "sample = dataset['train'][0]\n",
    "print(f\"\\nFull sample structure: {sample.keys()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(sample)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Create Dataset Subsets\n",
    "\n",
    "# %%\n",
    "# Create subsets\n",
    "print(f\"\\nCreating subsets...\")\n",
    "train_subset = dataset[\"train\"].select(range(min(TRAIN_SUBSET_SIZE, len(dataset[\"train\"]))))\n",
    "val_subset = dataset[\"validation\"].select(range(min(VAL_SUBSET_SIZE, len(dataset[\"validation\"]))))\n",
    "\n",
    "print(f\"Train subset: {len(train_subset)} samples\")\n",
    "print(f\"Validation subset: {len(val_subset)} samples\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Load Tokenizer and Model\n",
    "\n",
    "# %%\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# %%\n",
    "# Load base model with 4-bit quantization for memory efficiency\n",
    "print(\"Loading base model...\")\n",
    "print(\"‚ö†Ô∏è Note: Loading Sarvam-1 (2B parameters) - this may take a few minutes...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Move to MPS\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded. Parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Configure and Apply LoRA\n",
    "\n",
    "# %%\n",
    "# LoRA configuration - targeting more modules for better adaptation\n",
    "print(\"Configuring LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Slightly higher rank for 10B model\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target all attention projections\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Preprocessing Function\n",
    "\n",
    "# %%\n",
    "def tokenize_fn(examples):\n",
    "    \"\"\"\n",
    "    Tokenize with proper prompt masking for Sarvam-1.\n",
    "    Only train on the output (English translation), not the input prompt (Sanskrit).\n",
    "    \"\"\"\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for translation in examples['translation']:\n",
    "        # Extract Sanskrit and English\n",
    "        sn_text = str(translation['sn']).strip()\n",
    "        en_text = str(translation['en']).strip()\n",
    "        \n",
    "        # Create prompt in a format suitable for Sarvam-1\n",
    "        prompt = f\"Translate this Sanskrit shloka to English:\\n{sn_text}\\n\\nTranslation:\"\n",
    "        \n",
    "        # Tokenize prompt separately to get its length\n",
    "        prompt_tokens = tokenizer(prompt, add_special_tokens=True)\n",
    "        prompt_len = len(prompt_tokens[\"input_ids\"])\n",
    "        \n",
    "        # Create full sequence: prompt + output + eos\n",
    "        full_text = prompt + \" \" + en_text + tokenizer.eos_token\n",
    "        \n",
    "        # Tokenize full sequence\n",
    "        encoding = tokenizer(\n",
    "            full_text,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"][0].tolist()\n",
    "        attention_mask = encoding[\"attention_mask\"][0].tolist()\n",
    "        \n",
    "        # Create labels: mask prompt, keep output\n",
    "        labels = input_ids.copy()\n",
    "        \n",
    "        # Mask the prompt tokens\n",
    "        for i in range(min(prompt_len, len(labels))):\n",
    "            labels[i] = -100\n",
    "        \n",
    "        # Mask padding tokens\n",
    "        for i in range(len(labels)):\n",
    "            if attention_mask[i] == 0:\n",
    "                labels[i] = -100\n",
    "        \n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_masks,\n",
    "        \"labels\": all_labels\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Tokenize Datasets\n",
    "\n",
    "# %%\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_subset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=train_subset.column_names,\n",
    "    desc=\"Tokenizing training data\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_subset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=val_subset.column_names,\n",
    "    desc=\"Tokenizing validation data\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Tokenization complete\")\n",
    "print(f\"Tokenized train samples: {len(tokenized_train)}\")\n",
    "print(f\"Tokenized val samples: {len(tokenized_val)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Debug Tokenization (Optional)\n",
    "\n",
    "# %%\n",
    "# Check a few examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEBUGGING TOKENIZED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(3, len(tokenized_train))):\n",
    "    example = tokenized_train[i]\n",
    "    \n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    \n",
    "    non_masked = sum(1 for label in example['labels'] if label != -100)\n",
    "    masked = sum(1 for label in example['labels'] if label == -100)\n",
    "    padding = sum(1 for mask in example['attention_mask'] if mask == 0)\n",
    "    actual_tokens = len(example['input_ids']) - padding\n",
    "    \n",
    "    print(f\"Sequence length: {len(example['input_ids'])}\")\n",
    "    print(f\"Actual tokens (no padding): {actual_tokens}\")\n",
    "    print(f\"Padding tokens: {padding}\")\n",
    "    print(f\"Non-masked tokens (trainable): {non_masked}\")\n",
    "    print(f\"Masked tokens: {masked}\")\n",
    "    print(f\"Percentage trainable: {non_masked / len(example['labels']) * 100:.1f}%\")\n",
    "    \n",
    "    if non_masked == 0:\n",
    "        print(\"‚ö†Ô∏è WARNING: No trainable tokens!\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Setup Training\n",
    "\n",
    "# %%\n",
    "# Training arguments optimized for M4 Mac\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    warmup_steps=50,\n",
    "    fp16=True,  # MPS supports fp16\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=0,  # Important for MPS\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    ")\n",
    "\n",
    "print(\"üìù Training Configuration:\")\n",
    "print(f\"  - Model: {MODEL_NAME}\")\n",
    "print(f\"  - Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Gradient accumulation: {GRADIENT_ACCUM_STEPS}\")\n",
    "print(f\"  - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM_STEPS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Total epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "# %%\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Train the Model\n",
    "\n",
    "# %%\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Training complete!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 13. Save Model\n",
    "\n",
    "# %%\n",
    "# Save the final model\n",
    "print(f\"\\nSaving model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"‚úÖ Model saved successfully!\")\n",
    "print(f\"‚úì Location: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49b7f35-d677-4a56-9411-9da87ca3fe81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee65a2c4a3d45498af7e825b68350bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate in sanskrit: ‡§ß‡§∞‡•ç‡§Æ‡•ã ‡§∞‡§ï‡•ç‡§∑‡§§‡§ø ‡§∞‡§ï‡•ç‡§∑‡§ø‡§§‡§É ‡•§\n",
      "‡§® ‡§§‡§∞‡•ç‡§π‡§ø ‡§∞‡§ï‡•ç‡§∑‡§∏‡§æ ‡§∞‡§ï‡•ç‡§∑‡•ç‡§Ø‡§§‡•á ‡§∞‡§ï‡•ç‡§∑‡§∏‡§É ‡•§\n",
      "‡§∞‡§ï‡•ç‡§∑‡§æ‡§∞‡•ç‡§•‡§Ç ‡§π‡§ø ‡§§‡§§‡•ç ‡§∞‡§ï‡•ç‡§∑‡§§ ‡§∞‡§ï‡•ç‡§∑‡§∏‡§æ‡§Ç ‡§∞‡§ï‡•ç‡§∑‡§ø‡§§‡§æ‡§∞‡§É ‡••\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Sarvam-1 3B + LoRA ‚Äî CPU ONLY (STABLE, NO KERNEL DEATH)\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# -----------------------------\n",
    "# Force CPU everywhere\n",
    "# -----------------------------\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sarvamai/sarvam-1\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load base model (CPU, float16)\n",
    "# -----------------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"sarvamai/sarvam-1\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": \"cpu\"}\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Load LoRA adapter (CPU)\n",
    "# -----------------------------\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./sarvam1-itihasa-lora/\",\n",
    "    device_map={\"\": \"cpu\"}\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Inference\n",
    "# -----------------------------\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    \"‡§ß‡§∞‡•ç‡§Æ‡•ã ‡§∞‡§ï‡•ç‡§∑‡§§‡§ø ‡§∞‡§ï‡•ç‡§∑‡§ø‡§§‡§É\",\n",
    "    \"‡§∏‡§§‡•ç‡§Ø‡§Æ‡•á‡§µ ‡§ú‡§Ø‡§§‡•á\",\n",
    "    \"‡§µ‡§∏‡•Å‡§ß‡•à‡§µ ‡§ï‡•Å‡§ü‡•Å‡§Æ‡•ç‡§¨‡§ï‡§Æ‡•ç\",\n",
    "    \"‡§Ö‡§π‡§ø‡§Ç‡§∏‡§æ ‡§™‡§∞‡§Æ‡•ã ‡§ß‡§∞‡•ç‡§Æ‡§É\",\n",
    "]\n",
    "\n",
    "prompt = f\"translate in sanskrit: {test_samples[0]}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b742c9-1594-434a-b866-dce4f074ca6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate in sanskrit: ‡§Ö‡§π‡§ø‡§Ç‡§∏‡§æ ‡§™‡§∞‡§Æ‡•ã ‡§ß‡§∞‡•ç‡§Æ‡§É\n",
      "‡§Ö‡§π‡§ø‡§Ç‡§∏‡§æ ‡§™‡§∞‡§Æ‡§Ç ‡§ß‡§∞‡•ç‡§Æ‡§É (a-himsƒÅ paro dharma·∏•) literally means \"Ahimsa is the highest religion\". This phrase appears several times in the Tripitaka, especially in the Mahavagga and the Parivr√¢jakavagga of the Suttanipata. The first verse quoted here also uses the word Dharma as an alternative to 'ahimsa'. However,\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"translate in sanskrit: {test_samples[3]}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77d3408d-baa2-4ae8-9c49-2045be606f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§§‡§∏‡•ç‡§Ø‡§æ‡§Ç ‡§ö‡•Ä‡§∞‡§Ç ‡§µ‡§∏‡§æ‡§®‡§æ‡§Ø‡§æ‡§Ç ‡§®‡§æ‡§•‡§µ‡§§‡•ç‡§Ø‡§æ‡§Æ‡§®‡§æ‡§•‡§µ‡§§‡•ç‡•§ ‡§™‡•ç‡§∞‡§ö‡•Å‡§ï‡•ç‡§∞‡•ã‡§∂ ‡§ú‡§®‡§É ‡§∏‡§∞‡•ç‡§µ‡•ã ‡§ß‡§ø‡§ï‡•ç ‡§§‡•ç‡§µ‡§æ‡§Ç ‡§¶‡§∂‡§∞‡§•‡§Ç ‡§§‡•ç‡§µ‡§ø‡§§‡§ø ‡••\n",
      "\n",
      "üéØ Expected:\n",
      "When ≈ûƒ´tƒÅ, having a husband although seeming as if she had none, was putting on the ascetic guise, the people got into a wrath and exclaimed, ‚ÄúO Dasaratha, fie on you!\"\n",
      "\n",
      "‚ú® Model Output:\n",
      "The people, enraged by his arrogance, exclaimed, O Lord of men! This Nala is worse than your wife, O King.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§§‡•á‡§® ‡§§‡§§‡•ç‡§∞ ‡§™‡•ç‡§∞‡§£‡§æ‡§¶‡•á‡§® ‡§¶‡•Å‡§É‡§ñ‡§ø‡§§‡§É ‡§∏ ‡§Æ‡§π‡•Ä‡§™‡§§‡§ø‡§É‡•§ ‡§ö‡§ø‡§ö‡•ç‡§õ‡•á‡§¶ ‡§ú‡•Ä‡§µ‡§ø‡§§‡•á ‡§∂‡•ç‡§∞‡§¶‡•ç‡§ß‡§æ‡§Ç ‡§ß‡§∞‡•ç‡§Æ‡•á ‡§Ø‡§∂‡§∏‡§ø ‡§ö‡§æ‡§§‡•ç‡§Æ‡§®‡§É‡•• ‡§∏ ‡§®‡§ø‡§É‡§∂‡•ç‡§µ‡§∏‡•ç‡§Ø‡•ã‡§∑‡•ç‡§£‡§Æ‡•à‡§ï‡•ç‡§∑‡•ç‡§µ‡§æ‡§ï‡§∏‡•ç‡§§‡§æ‡§Ç ‡§≠‡§æ‡§∞‡•ç‡§Ø‡§æ‡§Æ‡§ø‡§¶‡§Æ‡§¨‡•ç‡§∞‡§µ‡•Ä‡§§‡•ç‡•§ ‡§ï‡•à‡§ï‡•á‡§Ø‡§ø ‡§ï‡•Å‡§∂‡§ö‡•Ä‡§∞‡•á‡§£ ‡§® ‡§∏‡•Ä‡§§‡§æ ‡§ó‡§®‡•ç‡§§‡•Å‡§Æ‡§∞‡•ç‡§π‡§§‡§ø‡••\n",
      "\n",
      "üéØ Expected:\n",
      "Aggrieved at the uproar that arose there in consequence, the lord of earth banished from his heart all regard for life, virtue, and fame. And sighing hot, that descendant of Ik≈üvƒÅku spoke to that wife of his, saying, O Kaikeyi, Sƒ´tƒÅ deserves not to go in a Kuƒáa dress.\n",
      "\n",
      "‚ú® Model Output:\n",
      "Thus, the king being saddened at that time and with disgusted mind thought within himself, \"How can I go on living in such a way? How is it possible for me to be able to meet my wife Ceyt√© here. (He) said unto his queen Ceyt√©, \"Come not to see us now.\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§∏‡•Å‡§ï‡•Å‡§Æ‡§æ‡§∞‡•Ä ‡§ö ‡§¨‡§æ‡§≤‡§æ ‡§ö ‡§∏‡§§‡§§‡§Ç ‡§ö ‡§∏‡•Å‡§ñ‡•ã‡§ö‡§ø‡§§‡§æ‡•§ ‡§®‡•á‡§Ø‡§Ç ‡§µ‡§®‡§∏‡•ç‡§Ø ‡§Ø‡•ã‡§ó‡•ç‡§Ø‡•á‡§§‡§ø ‡§∏‡§§‡•ç‡§Ø‡§Æ‡§æ‡§π ‡§ó‡•Å‡§∞‡•Å‡§∞‡•ç‡§Æ‡§Æ ‡••\n",
      "\n",
      "üéØ Expected:\n",
      "Tender, and youthful, and worthy of happiness, she is by no means capable of living in the forest. My spiritual guide has spoken the truth.\n",
      "\n",
      "‚ú® Model Output:\n",
      "The girl, child and always cheerful; what is this fit for the forest? I say it not.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§á‡§Ø‡§Ç ‡§π‡§ø ‡§ï‡§∏‡•ç‡§Ø‡§æ‡§™‡§ø ‡§ï‡§∞‡•ã‡§§‡§ø ‡§ï‡§ø‡§Ç‡§ö‡§ø‡§§‡•ç ‡§§‡§™‡§∏‡•ç‡§µ‡§ø‡§®‡•Ä ‡§∞‡§æ‡§ú‡§µ‡§∞‡§∏‡•ç‡§Ø ‡§™‡•Å‡§§‡•ç‡§∞‡•Ä‡•§ ‡§Ø‡§æ ‡§ö‡•Ä‡§∞‡§Æ‡§æ‡§∏‡§æ‡§¶‡•ç‡§Ø ‡§µ‡§®‡§∏‡•ç‡§Ø ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ú‡§æ‡§§‡§æ ‡§µ‡§ø‡§∏‡§Ç‡§ú‡•ç‡§û‡§æ ‡§∂‡•ç‡§∞‡§Æ‡§£‡•Ä‡§µ ‡§ï‡§æ‡§ö‡§ø‡§§‡•ç‡••\n",
      "\n",
      "üéØ Expected:\n",
      "Whom has this one injured that, being the daughter of the foremost of kings, she like a female ascetic, wearing a meagre garb in the presence of all, will (repair to the woods and) remain there like a beggar destitute of everything?\n",
      "\n",
      "‚ú® Model Output:\n",
      "This was the daughter of a royal sage, who went into the forest. She had no name, and she resembled one of those who are called 'sandal-worshippers.'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§ö‡•Ä‡§∞‡§æ‡§£‡•ç‡§Ø‡§™‡§æ‡§∏‡•ç‡§Ø‡§æ‡§ú‡•ç‡§ú‡§®‡§ï‡§∏‡•ç‡§Ø ‡§ï‡§®‡•ç‡§Ø‡§æ ‡§®‡•á‡§Ø‡§Ç ‡§™‡•ç‡§∞‡§§‡§ø‡§ú‡•ç‡§û‡§æ ‡§Æ‡§Æ ‡§¶‡§§‡•ç‡§§‡§™‡•Ç‡§∞‡•ç‡§µ‡§æ‡•§ ‡§Ø‡§•‡§æ‡§∏‡•Å‡§ñ‡§Ç ‡§ó‡§ö‡•ç‡§õ‡§§‡•Å ‡§∞‡§æ‡§ú‡§™‡•Å‡§§‡•ç‡§∞‡•Ä ‡§µ‡§®‡§Ç ‡§∏‡§Æ‡§ó‡•ç‡§∞‡§æ ‡§∏‡§π ‡§∏‡§∞‡•ç‡§µ‡§∞‡§§‡•ç‡§®‡•à‡§É‡••\n",
      "\n",
      "üéØ Expected:\n",
      "Let Janaka's daughter leave off her ascetic guise. This is not the promise that I had made to you before. Let the princess go to the forest in comfort, furnished with all sorts of gems.\n",
      "\n",
      "‚ú® Model Output:\n",
      "O King, I have passed by these women as a promise made to thee. Let me go and return with all my ladies of beauty in equal number.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 6\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§Ö‡§ú‡•Ä‡§µ‡§®‡§æ‡§π‡•á‡§£ ‡§Æ‡§Ø‡§æ ‡§®‡•É‡§∂‡§Ç‡§∏‡§æ ‡§ï‡•É‡§§‡§æ ‡§™‡•ç‡§∞‡§§‡§ø‡§ú‡•ç‡§û‡§æ ‡§®‡§ø‡§Ø‡§Æ‡•á‡§® ‡§§‡§æ‡§µ‡§§‡•ç‡•§ ‡§§‡•ç‡§µ‡§Ø‡§æ ‡§π‡§ø ‡§¨‡§æ‡§≤‡•ç‡§Ø‡§æ‡§§‡•ç ‡§™‡•ç‡§∞‡§§‡§ø‡§™‡§®‡•ç‡§®‡§Æ‡•á‡§§‡§§‡•ç ‡§§‡§®‡•ç‡§Æ‡§æ ‡§¶‡§π‡•á‡§¶‡•ç ‡§µ‡•á‡§£‡•Å‡§Æ‡§ø‡§µ‡§æ‡§§‡•ç‡§Æ‡§™‡•Å‡§∑‡•ç‡§™‡§Æ‡•ç‡••\n",
      "\n",
      "üéØ Expected:\n",
      "My sands run out; by me has this cruel promise been made with an oath. But this (exile of Sƒ´t√§) has been thought of by you through your ignorance. Let it not, however, consume you like a bamboo flower destroying the bamboo.\n",
      "\n",
      "‚ú® Model Output:\n",
      "He who has inflicted on me the curse of death by my own act, at that moment I would destroy myself as though a flower was being withered away.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 7\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§∞‡§æ‡§Æ‡•á‡§£ ‡§Ø‡§¶‡§ø ‡§§‡•á ‡§™‡§æ‡§™‡•á ‡§ï‡§ø‡§Ç‡§ö‡§ø‡§§‡•ç‡§ï‡•É‡§§‡§Æ‡§∂‡•ã‡§≠‡§®‡§Æ‡•ç‡•§ ‡§Ö‡§™‡§ï‡§æ‡§∞‡§É ‡§ï ‡§á‡§π ‡§§‡•á ‡§µ‡•à‡§¶‡•á‡§π‡•ç‡§Ø‡§æ ‡§¶‡§∞‡•ç‡§∂‡§ø‡§§‡•ã‡§Ω‡§ß‡§Æ‡•á‡••\n",
      "\n",
      "üéØ Expected:\n",
      "If, O wicked woman, RƒÅma has happened to do you something unbeautiful, what wrong, O base wretch, has Vaidehi done you in the world?\n",
      "\n",
      "‚ú® Model Output:\n",
      "If any evil, or unhonoured deed is done by you in the house of R√°ma, what can he be for whom such an act has been committed?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§Æ‡•É‡§ó‡•Ä‡§µ‡•ã‡§§‡•ç‡§´‡•Å‡§≤‡•ç‡§≤‡§®‡§Ø‡§®‡§æ ‡§Æ‡•É‡§¶‡•Å‡§∂‡•Ä‡§≤‡§æ ‡§Æ‡§®‡§∏‡•ç‡§µ‡§ø‡§®‡•Ä‡•§ ‡§Ö‡§™‡§ï‡§æ‡§∞‡§Ç ‡§ï‡§Æ‡§ø‡§µ ‡§§‡•á ‡§ï‡§∞‡•ã‡§§‡§ø ‡§ú‡§®‡§ï‡§æ‡§§‡•ç‡§Æ‡§ú‡§æ‡••\n",
      "\n",
      "üéØ Expected:\n",
      "Of eyes expanded like those of a doe, endued with a mild temperament, and virtuous, what harm has Janaka's daughter done you.\n",
      "\n",
      "‚ú® Model Output:\n",
      "O dear daughter of the king, you have a soft heart and are pleasant in speech. You do not injure others by any means; why then should your father be angry with you?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 9\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§®‡§®‡•Å ‡§™‡§∞‡•ç‡§Ø‡§æ‡§™‡•ç‡§§‡§Æ‡•á‡§µ‡§Ç ‡§§‡•á ‡§™‡§æ‡§™‡•á ‡§∞‡§æ‡§Æ‡§µ‡§ø‡§µ‡§æ‡§∏‡§®‡§Æ‡•ç‡•§ ‡§ï‡§ø‡§Æ‡•á‡§≠‡§ø‡§É ‡§ï‡•É‡§™‡§£‡•à‡§∞‡•ç‡§≠‡•Ç‡§Ø‡§É ‡§™‡§æ‡§§‡§ï‡•à‡§∞‡§™‡§ø ‡§§‡•á ‡§ï‡•É‡§§‡•à‡§É‡••\n",
      "\n",
      "üéØ Expected:\n",
      "Surely, O nefarious one, the banishment of R√°ma is enough for you. Why then do you bend your mind to perpetrate these atrocious sins?\n",
      "\n",
      "‚ú® Model Output:\n",
      "Oh Rama, you are so powerful that if even such a man as me has committed any sin, I have not done it by myself. It is all due to the wickedness of these very sins which have been perpetrated by many others in different directions too.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§™‡•ç‡§∞‡§§‡§ø‡§ú‡•ç‡§û‡§æ‡§§‡§Ç ‡§Æ‡§Ø‡§æ ‡§§‡§æ‡§µ‡§§‡•ç ‡§§‡•ç‡§µ‡§Ø‡•ã‡§ï‡•ç‡§§‡§Ç ‡§¶‡•á‡§µ‡§ø ‡§∂‡•É‡§£‡•ç‡§µ‡§§‡§æ‡•§ ‡§∞‡§æ‡§Æ‡§Ç ‡§Ø‡§¶‡§≠‡§ø‡§∑‡•á‡§ï‡§æ‡§Ø ‡§§‡•ç‡§µ‡§Æ‡§ø‡§π‡§æ‡§ó‡§§‡§Æ‡§¨‡•ç‡§∞‡§µ‡•Ä‡§É‡••\n",
      "\n",
      "üéØ Expected:\n",
      "O noble dame, having heard you asking for the banishment of RƒÅma, who had at first been intended by me for being installed, and who came here afterwards, I had promised you (his exile alone.)\n",
      "\n",
      "‚ú® Model Output:\n",
      "I have declared it to you, O dear. He who is going to be crowned by me with the royal anointment, come hither at once!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 11\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§§‡§§‡•ç‡§§‡•ç‡§µ‡•á‡§§‡§§‡•ç ‡§∏‡§Æ‡§§‡§ø‡§ï‡•ç‡§∞‡§Æ‡•ç‡§Ø ‡§®‡§ø‡§∞‡§Ø‡§Ç ‡§ó‡§®‡•ç‡§§‡•Å‡§Æ‡§ø‡§ö‡•ç‡§õ‡§∏‡§ø‡•§ ‡§Æ‡•à‡§•‡§ø‡§≤‡•Ä‡§Æ‡§™‡§ø ‡§Ø‡§æ ‡§π‡§ø ‡§§‡•ç‡§µ‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§∏‡•á ‡§ö‡•Ä‡§∞‡§µ‡§æ‡§∏‡§ø‡§®‡•Ä‡§Æ‡•ç‡••\n",
      "\n",
      "üéØ Expected:\n",
      "But since, going beyond that promise of mine, you behold Mithala's daughter dressed in mendicant garb, surely you wish to find your way to hell.\n",
      "\n",
      "‚ú® Model Output:\n",
      "May you, O PƒÅ·πá·∏çava, break through the bonds of rebirth and go forth from this world by wishing for Maitreyi's death.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 12\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§è‡§µ‡§Ç ‡§¨‡•ç‡§∞‡•Å‡§µ‡§®‡•ç‡§§‡§Ç ‡§™‡§ø‡§§‡§∞‡§Ç ‡§∞‡§æ‡§Æ‡§É ‡§∏‡§Æ‡•ç‡§™‡•ç‡§∞‡§∏‡•ç‡§•‡§ø‡§§‡•ã ‡§µ‡§®‡§Æ‡•ç‡•§ ‡§Ö‡§µ‡§æ‡§ï‡•ç‡§∂‡§ø‡§∞‡§∏‡§Æ‡§æ‡§∏‡•Ä‡§®‡§Æ‡§ø‡§¶‡§Ç ‡§µ‡§ö‡§®‡§Æ‡§¨‡•ç‡§∞‡§µ‡•Ä‡§§‡•ç‡••\n",
      "\n",
      "üéØ Expected:\n",
      "Thus commissioned to the forest, R√§ma who was seated sealing his lips, said.\n",
      "\n",
      "‚ú® Model Output:\n",
      "Then, Rama said unto his father: O Bharata, when you were in the forest, you did not speak a word.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 13\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§á‡§Ø‡§Ç ‡§ß‡§æ‡§∞‡•ç‡§Æ‡§ø‡§ï ‡§ï‡•å‡§∏‡§≤‡•ç‡§Ø‡§æ ‡§Æ‡§Æ ‡§Æ‡§æ‡§§‡§æ ‡§Ø‡§∂‡§∏‡•ç‡§µ‡§ø‡§®‡•Ä‡•§ ‡§µ‡•É‡§¶‡•ç‡§ß‡§æ ‡§ö‡§æ‡§ï‡•ç‡§∑‡•Å‡§¶‡•ç‡§∞‡§∂‡•Ä‡§≤‡§æ ‡§ö ‡§® ‡§ö ‡§§‡•ç‡§µ‡§æ‡§Ç ‡§¶‡•á‡§µ ‡§ó‡§∞‡•ç‡§π‡§§‡•á‡•• ‡§Æ‡§Ø‡§æ ‡§µ‡§ø‡§π‡•Ä‡§®‡§æ‡§Ç ‡§µ‡§∞‡§¶ ‡§™‡•ç‡§∞‡§™‡§®‡•ç‡§®‡§æ‡§Ç ‡§∂‡•ã‡§ï‡§∏‡§æ‡§ó‡§∞‡§Æ‡•ç‡•§ ‡§Ö‡§¶‡•É‡§∑‡•ç‡§ü‡§™‡•Ç‡§∞‡•ç‡§µ‡§µ‡•ç‡§Ø‡§∏‡§®‡§æ‡§Ç ‡§≠‡•Ç‡§Ø‡§É ‡§∏‡§Æ‡•ç‡§Æ‡§®‡•ç‡§§‡•Å‡§Æ‡§∞‡•ç‡§π‡§∏‡§ø ‡••\n",
      "\n",
      "üéØ Expected:\n",
      "O righteous one, this my mother is aged and famous and of lofty spirit. May she not meet with improper treatment at your hands! It behoves you, O bestower of boons, to show greater honour to her when she shall be deprived of me and be plunged into a sea of grief and afflicted with unprecedented woe.\n",
      "\n",
      "‚ú® Model Output:\n",
      "This cousin is my mother, the daughter of Yayati. She is also old and has no children but I was praying for her. O Lord, take pity on me, deliver me from this misery. May you have many sons.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 14\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§™‡•Å‡§§‡•ç‡§∞‡§∂‡•ã‡§ï‡§Ç ‡§Ø‡§•‡§æ ‡§®‡§õ‡•á‡§§‡•ç ‡§§‡•ç‡§µ‡§Ø‡§æ ‡§™‡•Ç‡§ú‡•ç‡§Ø‡•á‡§® ‡§™‡•Ç‡§ú‡§ø‡§§‡§æ‡•§ ‡§Æ‡§æ‡§Ç ‡§π‡§ø ‡§∏‡§Ç‡§ö‡§ø‡§®‡•ç‡§§‡§Ø‡§®‡•ç‡§§‡•Ä ‡§∏‡§æ ‡§§‡•ç‡§µ‡§Ø‡§ø ‡§ú‡•Ä‡§µ‡•á‡§§‡•ç ‡§§‡§™‡§∏‡•ç‡§µ‡§ø‡§®‡•Ä‡•• ‡§á‡§Æ‡§æ‡§Ç ‡§Æ‡§π‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡•ã‡§™‡§Æ ‡§ú‡§æ‡§§‡§ó‡§∞‡•ç‡§ß‡§ø‡§®‡•Ä ‡§§‡§•‡§æ ‡§µ‡§ø‡§ß‡§æ‡§§‡•Å‡§Ç ‡§ú‡§®‡§®‡•Ä ‡§Æ‡§Æ‡§æ‡§∞‡•ç‡§π‡§∏‡§ø‡•§ ‡§Ø‡§•‡§æ ‡§µ‡§®‡§∏‡•ç‡§•‡•á ‡§Æ‡§Ø‡§ø ‡§∂‡•ã‡§ï‡§ï‡§∞‡•ç‡§∂‡§ø‡§§‡§æ ‡§® ‡§ú‡•Ä‡§µ‡§ø‡§§‡§Ç ‡§®‡•ç‡§Ø‡§∏‡•ç‡§Ø ‡§Ø‡§Æ‡§ï‡•ç‡§∑‡§Ø‡§Ç ‡§µ‡•ç‡§∞‡§ú‡•á‡§§‡•ç‡••\n",
      "\n",
      "üéØ Expected:\n",
      "O you comparable to the mighty Indra, you should so behove with my mother smitten with my separation, that exercised by grief in consequence of my residence in the forest, she may not, renouncing life, repair to the mansions of Yama.\n",
      "\n",
      "‚ú® Model Output:\n",
      "You, O lord of the gods, should not give up the grief which you have caused me by killing your son. For she is a devoted wife who thinks of nothing else but my welfare and will live with me as long as I am in her power. Just like Indra's daughter who would be born into that great race-stallion (garden), so do thou cause me to be born again. Do not let thy mind be disturbed for evermore on account of my death.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 15\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§Æ‡•Å‡§®‡•á‡§∞‡•ç‡§µ‡§ö‡§®‡§Æ‡§ï‡•ç‡§≤‡•Ä‡§¨‡§Ç ‡§∂‡•ç‡§∞‡•Å‡§§‡•ç‡§µ‡§æ ‡§®‡§∞‡§µ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ú‡§É‡•§ ‡§∞‡§æ‡§ò‡§µ‡§É ‡§™‡•ç‡§∞‡§æ‡§û‡•ç‡§ú‡§≤‡§ø‡§∞‡•ç‡§≠‡•Ç‡§§‡•ç‡§µ‡§æ ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•Å‡§µ‡§æ‡§ö ‡§¶‡•É‡§¢‡§µ‡•ç‡§∞‡§§‡§É‡••\n",
      "\n",
      "üéØ Expected:\n",
      "Hearing those bold words of the ascetic, the son of that foremost of men, RƒÅghava firm in his vows, with clasped hands answered.\n",
      "\n",
      "‚ú® Model Output:\n",
      "\"Thereafter, when the minister and others heard what was said by Rama, he straightway saluted him with folded hands.\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 16\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§™‡§ø‡§§‡•Å‡§∞‡•ç‡§µ‡§ö‡§®‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂‡§æ‡§§‡•ç ‡§™‡§ø‡§§‡•Å‡§∞‡•ç‡§µ‡§ö‡§®‡§ó‡•å‡§∞‡§µ‡§æ‡§§‡•ç‡•§ ‡§µ‡§ö‡§®‡§Ç ‡§ï‡•å‡§∂‡§ø‡§ï‡§∏‡•ç‡§Ø‡•á‡§§‡§ø ‡§ï‡§∞‡•ç‡§§‡§µ‡•ç‡§Ø‡§Æ‡§µ‡§ø‡§∂‡§ô‡•ç‡§ï‡§Ø‡§æ‡•• ‡§Ö‡§®‡•Å‡§∂‡§ø‡§∑‡•ç‡§ü‡•ã‡§Ω‡§∏‡•ç‡§Æ‡•ç‡§Ø‡§Ø‡•ã‡§ß‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Ç ‡§ó‡•Å‡§∞‡•Å‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§Æ‡§π‡§æ‡§§‡•ç‡§Æ‡§®‡§æ‡•§ ‡§™‡§ø‡§§‡•ç‡§∞‡§æ ‡§¶‡§∂‡§∞‡§•‡•á‡§®‡§æ‡§π‡§Ç ‡§®‡§æ‡§µ‡§ú‡•ç‡§û‡•á‡§Ø‡§Ç ‡§π‡§ø ‡§§‡§¶‡•ç‡§µ‡§ö‡§É‡••\n",
      "\n",
      "üéØ Expected:\n",
      "In accordance with the desire of my sire, and in order to glorify it, I ought fearlessly to do even as Kusika's son says. And having been desired to that end while at AyodhyƒÅ by that high-souled one, my father Dasaratha, in the midst of the spiritual guides, I ought not to pass by your words.\n",
      "\n",
      "‚ú® Model Output:\n",
      "I have been told by my father's words that he has appointed me as a teacher of the army. Now, having made up my mind and not doubting his word, I shall go to Uttara-kaushala with your permission.\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 17\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§∏‡•ã‡§Ω‡§π‡§Ç ‡§™‡§ø‡§§‡•Å‡§∞‡•ç‡§µ‡§ö‡§É ‡§∂‡•ç‡§∞‡•Å‡§§‡•ç‡§µ‡§æ ‡§∂‡§æ‡§∏‡§®‡§æ‡§¶‡•ç ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§µ‡§æ‡§¶‡§ø‡§®‡§É‡•§ ‡§ï‡§∞‡§ø‡§∑‡•ç‡§Ø‡§æ‡§Æ‡§ø ‡§® ‡§∏‡§Ç‡§¶‡•á‡§π‡§∏‡•ç‡§§‡§æ‡§ü‡§ï‡§æ‡§µ‡§ß‡§Æ‡•Å‡§§‡•ç‡§§‡§Æ‡§Æ‡•ç‡••\n",
      "\n",
      "üéØ Expected:\n",
      "Therefore, commanded by that upholder of the Veda, I, agreeably to my father's mandate, will, without doubt, bring about that welcome event-the death of Tataka.\n",
      "\n",
      "‚ú® Model Output:\n",
      "I, having heard the words of my father, will be performing a sacrifice in which I shall slay him.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 18\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§ó‡•ã‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£‡§π‡§ø‡§§‡§æ‡§∞‡•ç‡§•‡§æ‡§Ø ‡§¶‡•á‡§∂‡§∏‡•ç‡§Ø ‡§ö ‡§π‡§ø‡§§‡§æ‡§Ø ‡§ö‡•§ ‡§§‡§µ ‡§ö‡•à‡§µ‡§æ‡§™‡•ç‡§∞‡§Æ‡•á‡§Ø‡§∏‡•ç‡§Ø ‡§µ‡§ö‡§®‡§Ç ‡§ï‡§∞‡•ç‡§§‡•Å‡§Æ‡•Å‡§¶‡•ç‡§Ø‡§§‡§É‡••\n",
      "\n",
      "üéØ Expected:\n",
      "And in the interests of BrƒÅhma·πáas, kine, and celestials, I am ready to act as desired by you of immeasurable energy.\n",
      "\n",
      "‚ú® Model Output:\n",
      "For the welfare of the Brahmanas and the interest of country also, I have said it.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Sample 19\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§è‡§µ‡§Æ‡•Å‡§ï‡•ç‡§§‡•ç‡§µ‡§æ ‡§ß‡§®‡•Å‡§∞‡•ç‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§¨‡§¶‡•ç‡§ß‡•ç‡§µ‡§æ ‡§Æ‡•Å‡§∑‡•ç‡§ü‡§ø‡§Æ‡§∞‡§ø‡§Ç‡§¶‡§Æ‡§É‡•§ ‡§ú‡•ç‡§Ø‡§æ‡§ò‡•ã‡§∑‡§Æ‡§ï‡§∞‡•ã‡§§‡•ç ‡§§‡•Ä‡§µ‡•ç‡§∞ ‡§¶‡§ø‡§∂‡§É ‡§∂‡§¨‡•ç‡§¶‡•á‡§® ‡§®‡§æ‡§¶‡§Ø‡§®‡•ç ‡•• ‡§§‡•á‡§® ‡§∂‡§¨‡•ç‡§¶‡•á‡§® ‡§µ‡§ø‡§µ‡§∏‡•ç‡§§‡§æ‡§∏‡•ç‡§§‡§æ‡§ü‡§ï‡§æ‡§µ‡§®‡§µ‡§æ‡§∏‡§ø‡§®‡§É‡•§ ‡§§‡§æ‡§ü‡§ï‡§æ ‡§ö ‡§∏‡•Å‡§∏‡§Ç‡§ï‡•ç‡§∞‡•Å‡§¶‡•ç‡§ß‡§æ ‡§§‡•á‡§® ‡§∂‡§¨‡•ç‡§¶‡•á‡§® ‡§Æ‡•ã‡§π‡§ø‡§§‡§æ‡••\n",
      "\n",
      "üéØ Expected:\n",
      "Having said this, that repressor of foes, with clenched fist, twanged his bow-string, filling the ten cardinal points with the sounds. And at those sounds, the dwellers in TƒÅ»õakƒÅ's forest were filled with perturbation, and TƒÅtakƒÅ also amazed at those sounds, became exceedingly wroth.\n",
      "\n",
      "‚ú® Model Output:\n",
      "So saying, he bound his bow in the middle and hurled a mace at the sky. The voice of thunder sounding from afar filled with wonderment all those inside the tataka (town).\n",
      "\n",
      "====================================================================================================\n",
      "Sample 20\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üìú Sanskrit:\n",
      "‡§§‡§Ç ‡§∂‡§¨‡•ç‡§¶‡§Æ‡§≠‡§ø‡§®‡§ø‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§∞‡§æ‡§ï‡•ç‡§∑‡§∏‡•Ä ‡§ï‡•ç‡§∞‡•ã‡§ß‡§Æ‡•Ç‡§∞‡•ç‡§õ‡§ø‡§§‡§æ‡•§ ‡§∂‡•ç‡§∞‡•Å‡§§‡•ç‡§µ‡§æ ‡§ö‡§æ‡§≠‡•ç‡§Ø‡§¶‡•ç‡§∞‡§µ‡§§‡•ç ‡§ï‡•ç‡§∞‡•Å‡§¶‡•ç‡§ß‡§æ ‡§Ø‡§§‡•ç‡§∞ ‡§∂‡§¨‡•ç‡§¶‡•ã ‡§µ‡§ø‡§®‡§ø‡§É‡§∏‡•É‡§§‡§É‡••\n",
      "\n",
      "üéØ Expected:\n",
      "And rendered almost insensible by anger, that R√§ksasƒ´ furiously rushed in a main towards the spot whence had come the report.\n",
      "\n",
      "‚ú® Model Output:\n",
      "The demoness, overcome with fury, and on hearing the sound of his name, fell into a swoon.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_samples = 20\n",
    "\n",
    "for i in range(min(num_samples, len(val_subset))):\n",
    "    sample = val_subset[i]\n",
    "\n",
    "    sanskrit_text = sample[\"translation\"][\"sn\"]\n",
    "    expected_en = sample[\"translation\"][\"en\"]\n",
    "\n",
    "    prompt = (\n",
    "        \"Translate this Sanskrit shloka to English:\\n\"\n",
    "        f\"{sanskrit_text}\\n\\n\"\n",
    "        \"Translation:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"Translation:\" in decoded:\n",
    "        generated_en = decoded.split(\"Translation:\")[-1].strip()\n",
    "    else:\n",
    "        generated_en = decoded.strip()\n",
    "\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Sample {i + 1}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    print(\"üìú Sanskrit:\")\n",
    "    print(sanskrit_text)\n",
    "\n",
    "    print(\"\\nüéØ Expected:\")\n",
    "    print(expected_en)\n",
    "\n",
    "    print(\"\\n‚ú® Model Output:\")\n",
    "    print(generated_en)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8854f1c3-a839-4ccd-98f5-69e85f95c96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen-env",
   "language": "python",
   "name": "qwen-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
