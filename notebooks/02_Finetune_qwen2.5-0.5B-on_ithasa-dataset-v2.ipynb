{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7de6568-6994-4b00-93fa-f6fe3d682abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training subset: 20000 samples\n",
      "Validation subset: 500 samples\n",
      "Max sequence length: 256\n",
      "Effective batch size: 32\n",
      "Loading Itihasa dataset from local parquet files...\n",
      "\n",
      "Dataset structure:\n",
      "Train samples: 75162\n",
      "Validation samples: 6149\n",
      "Test samples: 11722\n",
      "\n",
      "================================================================================\n",
      "Sample from dataset:\n",
      "================================================================================\n",
      "\n",
      "Full sample structure: dict_keys(['translation'])\n",
      "\n",
      "Sample data:\n",
      "{'translation': {'en': 'The ascetic VÄlmÄ«ki asked NÄrada, the best of sages and foremost of those conversant with words, ever engaged in austerities and Vedic studies.', 'sn': 'à¥ à¤¤à¤ªà¤ƒ à¤¸à¥à¤µà¤¾à¤§à¥à¤¯à¤¾à¤¯à¤¨à¤¿à¤°à¤¤à¤‚ à¤¤à¤ªà¤¸à¥à¤µà¥€ à¤µà¤¾à¤—à¥à¤µà¤¿à¤¦à¤¾à¤‚ à¤µà¤°à¤®à¥à¥¤ à¤¨à¤¾à¤°à¤¦à¤‚ à¤ªà¤°à¤¿à¤ªà¤ªà¥à¤°à¤šà¥à¤› à¤µà¤¾à¤²à¥à¤®à¥€à¤•à¤¿à¤°à¥à¤®à¥à¤¨à¤¿à¤ªà¥à¤™à¥à¤—à¤µà¤®à¥à¥¥'}}\n",
      "\n",
      "Creating subsets...\n",
      "Train subset: 20000 samples\n",
      "Validation subset: 500 samples\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded. Vocab size: 151665\n",
      "Loading base model...\n",
      "Model loaded. Parameters: 494,032,768\n",
      "Configuring LoRA...\n",
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967132cb03fa4187a22e0cd0a704fa40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing training data:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenization complete\n",
      "Tokenized train samples: 20000\n",
      "Tokenized val samples: 500\n",
      "\n",
      "================================================================================\n",
      "DEBUGGING TOKENIZED DATA\n",
      "================================================================================\n",
      "\n",
      "--- Example 0 ---\n",
      "Sequence length: 256\n",
      "Actual tokens (no padding): 136\n",
      "Padding tokens: 120\n",
      "Non-masked tokens (trainable): 40\n",
      "Masked tokens: 216\n",
      "Percentage trainable: 15.6%\n",
      "\n",
      "--- Example 1 ---\n",
      "Sequence length: 256\n",
      "Actual tokens (no padding): 134\n",
      "Padding tokens: 122\n",
      "Non-masked tokens (trainable): 31\n",
      "Masked tokens: 225\n",
      "Percentage trainable: 12.1%\n",
      "\n",
      "--- Example 2 ---\n",
      "Sequence length: 256\n",
      "Actual tokens (no padding): 129\n",
      "Padding tokens: 127\n",
      "Non-masked tokens (trainable): 35\n",
      "Masked tokens: 221\n",
      "Percentage trainable: 13.7%\n",
      "================================================================================\n",
      "ğŸ“ Training Configuration:\n",
      "  - Max sequence length: 256\n",
      "  - Batch size: 8\n",
      "  - Gradient accumulation: 4\n",
      "  - Effective batch size: 32\n",
      "  - Learning rate: 1e-05\n",
      "  - Total epochs: 1\n",
      "Trainer initialized successfully!\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ Starting training...\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 1:51:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.054500</td>\n",
       "      <td>4.043245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.986000</td>\n",
       "      <td>3.963691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.867300</td>\n",
       "      <td>3.895544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.870100</td>\n",
       "      <td>3.843967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.812100</td>\n",
       "      <td>3.809147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.785700</td>\n",
       "      <td>3.784872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.765400</td>\n",
       "      <td>3.767917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.739200</td>\n",
       "      <td>3.756110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.739700</td>\n",
       "      <td>3.747189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.748800</td>\n",
       "      <td>3.741186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.723600</td>\n",
       "      <td>3.737294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.720600</td>\n",
       "      <td>3.735564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shreekrishnajamakatel/personal/llm/llm_model_test/qwen-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… Training complete!\n",
      "================================================================================\n",
      "\n",
      "Saving model to ./qwen-itihasa-lora...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved successfully!\n",
      "âœ“ Location: ./qwen-itihasa-lora\n",
      "\n",
      "================================================================================\n",
      "Testing the trained model...\n",
      "================================================================================\n",
      "\n",
      "ğŸ“œ Sanskrit: à¤§à¤°à¥à¤®à¥‹ à¤°à¤•à¥à¤·à¤¤à¤¿ à¤°à¤•à¥à¤·à¤¿à¤¤à¤ƒ\n",
      "\n",
      "ğŸ”„ Generated output:\n",
      "Translate this Sanskrit shloka to English:\n",
      "à¤§à¤°à¥à¤®à¥‹ à¤°à¤•à¥à¤·à¤¤à¤¿ à¤°à¤•à¥à¤·à¤¿à¤¤à¤ƒ\n",
      "\n",
      "Translation: The ruler was overcome by fear and was unable to speak.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # LoRA Finetuning: Qwen2.5-0.5B on Itihasa Dataset\n",
    "# Optimized for Apple Silicon (M4 Mac) - Working Version\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Check for MPS availability\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Configuration\n",
    "\n",
    "# %%\n",
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "OUTPUT_DIR = \"./qwen-itihasa-lora\"\n",
    "\n",
    "# Dataset subset configuration\n",
    "TRAIN_SUBSET_SIZE = 20000  # Use first 1000 samples for training\n",
    "VAL_SUBSET_SIZE = 500     # Use first 200 samples for validation\n",
    "\n",
    "# Training configuration\n",
    "MAX_SEQ_LENGTH = 256\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 8  # Reduced for stability\n",
    "GRADIENT_ACCUM_STEPS = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "print(f\"Training subset: {TRAIN_SUBSET_SIZE} samples\")\n",
    "print(f\"Validation subset: {VAL_SUBSET_SIZE} samples\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM_STEPS}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Load Dataset from Local Parquet Files\n",
    "\n",
    "# %%\n",
    "# Load dataset from local parquet files\n",
    "print(\"Loading Itihasa dataset from local parquet files...\")\n",
    "dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\n",
    "        \"train\": \"ithasa/train/0000.parquet\",\n",
    "        \"validation\": \"ithasa/val/0000.parquet\",\n",
    "        \"test\": \"ithasa/test/0000.parquet\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Explore Dataset\n",
    "\n",
    "# %%\n",
    "# Show sample data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample from dataset:\")\n",
    "print(\"=\"*80)\n",
    "sample = dataset['train'][0]\n",
    "print(f\"\\nFull sample structure: {sample.keys()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(sample)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Create Dataset Subsets\n",
    "\n",
    "# %%\n",
    "# Create subsets\n",
    "print(f\"\\nCreating subsets...\")\n",
    "train_subset = dataset[\"train\"].select(range(min(TRAIN_SUBSET_SIZE, len(dataset[\"train\"]))))\n",
    "val_subset = dataset[\"validation\"].select(range(min(VAL_SUBSET_SIZE, len(dataset[\"validation\"]))))\n",
    "\n",
    "print(f\"Train subset: {len(train_subset)} samples\")\n",
    "print(f\"Validation subset: {len(val_subset)} samples\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Load Tokenizer and Model\n",
    "\n",
    "# %%\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# %%\n",
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model loaded. Parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Configure and Apply LoRA\n",
    "\n",
    "# %%\n",
    "# LoRA configuration\n",
    "print(\"Configuring LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Preprocessing Function\n",
    "\n",
    "# %%\n",
    "def tokenize_fn(examples):\n",
    "    \"\"\"\n",
    "    Tokenize with proper prompt masking.\n",
    "    Only train on the output (English translation), not the input prompt (Sanskrit).\n",
    "    \"\"\"\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for translation in examples['translation']:\n",
    "        # Extract Sanskrit and English\n",
    "        sn_text = str(translation['sn']).strip()\n",
    "        en_text = str(translation['en']).strip()\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"Translate this Sanskrit shloka to English:\\n{sn_text}\\n\\nTranslation:\"\n",
    "        \n",
    "        # Tokenize prompt separately to get its length\n",
    "        prompt_tokens = tokenizer(prompt, add_special_tokens=True)\n",
    "        prompt_len = len(prompt_tokens[\"input_ids\"])\n",
    "        \n",
    "        # Create full sequence: prompt + output + eos\n",
    "        full_text = prompt + \" \" + en_text + tokenizer.eos_token\n",
    "        \n",
    "        # Tokenize full sequence\n",
    "        encoding = tokenizer(\n",
    "            full_text,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"][0].tolist()\n",
    "        attention_mask = encoding[\"attention_mask\"][0].tolist()\n",
    "        \n",
    "        # Create labels: mask prompt, keep output\n",
    "        labels = input_ids.copy()\n",
    "        \n",
    "        # Mask the prompt tokens\n",
    "        for i in range(min(prompt_len, len(labels))):\n",
    "            labels[i] = -100\n",
    "        \n",
    "        # Mask padding tokens\n",
    "        for i in range(len(labels)):\n",
    "            if attention_mask[i] == 0:\n",
    "                labels[i] = -100\n",
    "        \n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_masks,\n",
    "        \"labels\": all_labels\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Tokenize Datasets\n",
    "\n",
    "# %%\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_subset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=train_subset.column_names,\n",
    "    desc=\"Tokenizing training data\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_subset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=val_subset.column_names,\n",
    "    desc=\"Tokenizing validation data\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Tokenization complete\")\n",
    "print(f\"Tokenized train samples: {len(tokenized_train)}\")\n",
    "print(f\"Tokenized val samples: {len(tokenized_val)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Debug Tokenization (Optional)\n",
    "\n",
    "# %%\n",
    "# Check a few examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEBUGGING TOKENIZED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(3, len(tokenized_train))):\n",
    "    example = tokenized_train[i]\n",
    "    \n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    \n",
    "    non_masked = sum(1 for label in example['labels'] if label != -100)\n",
    "    masked = sum(1 for label in example['labels'] if label == -100)\n",
    "    padding = sum(1 for mask in example['attention_mask'] if mask == 0)\n",
    "    actual_tokens = len(example['input_ids']) - padding\n",
    "    \n",
    "    print(f\"Sequence length: {len(example['input_ids'])}\")\n",
    "    print(f\"Actual tokens (no padding): {actual_tokens}\")\n",
    "    print(f\"Padding tokens: {padding}\")\n",
    "    print(f\"Non-masked tokens (trainable): {non_masked}\")\n",
    "    print(f\"Masked tokens: {masked}\")\n",
    "    print(f\"Percentage trainable: {non_masked / len(example['labels']) * 100:.1f}%\")\n",
    "    \n",
    "    if non_masked == 0:\n",
    "        print(\"âš ï¸ WARNING: No trainable tokens!\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Setup Training\n",
    "\n",
    "# %%\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    warmup_steps=25,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Training Configuration:\")\n",
    "print(f\"  - Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Gradient accumulation: {GRADIENT_ACCUM_STEPS}\")\n",
    "print(f\"  - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM_STEPS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Total epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "# %%\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Train the Model\n",
    "\n",
    "# %%\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Training complete!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 13. Save Model\n",
    "\n",
    "# %%\n",
    "# Save the final model\n",
    "print(f\"\\nSaving model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"âœ… Model saved successfully!\")\n",
    "print(f\"âœ“ Location: {OUTPUT_DIR}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 14. Quick Test\n",
    "\n",
    "# %%\n",
    "# Test the model with a sample\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing the trained model...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_sanskrit = \"à¤§à¤°à¥à¤®à¥‹ à¤°à¤•à¥à¤·à¤¤à¤¿ à¤°à¤•à¥à¤·à¤¿à¤¤à¤ƒ\"\n",
    "test_prompt = f\"Translate this Sanskrit shloka to English:\\n{test_sanskrit}\\n\\nTranslation:\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nğŸ“œ Sanskrit: {test_sanskrit}\")\n",
    "print(f\"\\nğŸ”„ Generated output:\\n{generated_text}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Done!\n",
    "# Your LoRA adapter is now trained and saved. Load it later using:\n",
    "# ```python\n",
    "# from peft import PeftModel\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "# model = PeftModel.from_pretrained(base_model, \"./qwen-itihasa-lora\")\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "715c8541-c7f4-47a1-8734-d16c651241ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading base model...\n",
      "Loading LoRA adapter...\n",
      "Loading tokenizer...\n",
      "âœ… Model loaded successfully!\n",
      "\n",
      "Loaded 10 test samples\n",
      "âœ… Translation function ready\n",
      "====================================================================================================\n",
      "SANSKRIT TO ENGLISH TRANSLATION TESTS\n",
      "====================================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 1/10: Mahabharata - Bhagavad Gita 1.1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“œ Sanskrit:\n",
      "   à¤§à¥ƒà¤¤à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤‰à¤µà¤¾à¤šà¥¤ à¤§à¤°à¥à¤®à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‡ à¤•à¥à¤°à¥à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‡ à¤¸à¤®à¤µà¥‡à¤¤à¤¾ à¤¯à¥à¤¯à¥à¤¤à¥à¤¸à¤µà¤ƒà¥¤ à¤®à¤¾à¤®à¤•à¤¾à¤ƒ à¤ªà¤¾à¤£à¥à¤¡à¤µà¤¾à¤¶à¥à¤šà¥ˆà¤µ à¤•à¤¿à¤®à¤•à¥à¤°à¥à¤µà¤¤ à¤¸à¤à¥à¤œà¤¯à¥¥\n",
      "\n",
      "ğŸ¯ Expected Context:\n",
      "   Dhritarashtra's opening question about the battle\n",
      "\n",
      "ğŸ”„ Translating...\n",
      "\n",
      "âœ¨ Model Translation:\n",
      "   The DhrÌ„taÅ›á¹­rÄ are the two parts that are situated in the earth. They are the two parts that are situated in the eastern and the western seas; they are also the two parts that are situated in the two continents.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 2/10: Mahabharata - Bhagavad Gita 4.7\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“œ Sanskrit:\n",
      "   à¤¯à¤¦à¤¾ à¤¯à¤¦à¤¾ à¤¹à¤¿ à¤§à¤°à¥à¤®à¤¸à¥à¤¯ à¤—à¥à¤²à¤¾à¤¨à¤¿à¤°à¥à¤­à¤µà¤¤à¤¿ à¤­à¤¾à¤°à¤¤à¥¤ à¤…à¤­à¥à¤¯à¥à¤¤à¥à¤¥à¤¾à¤¨à¤®à¤§à¤°à¥à¤®à¤¸à¥à¤¯ à¤¤à¤¦à¤¾à¤¤à¥à¤®à¤¾à¤¨à¤‚ à¤¸à¥ƒà¤œà¤¾à¤®à¥à¤¯à¤¹à¤®à¥à¥¥\n",
      "\n",
      "ğŸ¯ Expected Context:\n",
      "   Krishna on divine incarnation\n",
      "\n",
      "ğŸ”„ Translating...\n",
      "\n",
      "âœ¨ Model Translation:\n",
      "   I have recited to you the words of the Dharma-sage, and those words are the words of the Dharma-sage.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 3/10: Mahabharata - Bhagavad Gita 2.47\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“œ Sanskrit:\n",
      "   à¤•à¤°à¥à¤®à¤£à¥à¤¯à¥‡à¤µà¤¾à¤§à¤¿à¤•à¤¾à¤°à¤¸à¥à¤¤à¥‡ à¤®à¤¾ à¤«à¤²à¥‡à¤·à¥ à¤•à¤¦à¤¾à¤šà¤¨à¥¤ à¤®à¤¾ à¤•à¤°à¥à¤®à¤«à¤²à¤¹à¥‡à¤¤à¥à¤°à¥à¤­à¥‚à¤°à¥à¤®à¤¾ à¤¤à¥‡ à¤¸à¤™à¥à¤—à¥‹à¤½à¤¸à¥à¤¤à¥à¤µà¤•à¤°à¥à¤®à¤£à¤¿à¥¥\n",
      "\n",
      "ğŸ¯ Expected Context:\n",
      "   Famous verse on karma yoga\n",
      "\n",
      "ğŸ”„ Translating...\n",
      "\n",
      "âœ¨ Model Translation:\n",
      "   With the aid of a great brahmana, they [the people] learned all the Vedas.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 4/10: Mahabharata - Udyoga Parva\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“œ Sanskrit:\n",
      "   à¤¸à¤¤à¥à¤¯à¤‚ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤ªà¥à¤°à¤¿à¤¯à¤‚ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤¨ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤¸à¤¤à¥à¤¯à¤®à¤ªà¥à¤°à¤¿à¤¯à¤®à¥à¥¤ à¤ªà¥à¤°à¤¿à¤¯à¤‚ à¤š à¤¨à¤¾à¤¨à¥ƒà¤¤à¤‚ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤à¤· à¤§à¤°à¥à¤®à¤ƒ à¤¸à¤¨à¤¾à¤¤à¤¨à¤ƒà¥¥\n",
      "\n",
      "ğŸ¯ Expected Context:\n",
      "   Principles of truthful speech\n",
      "\n",
      "ğŸ”„ Translating...\n",
      "\n",
      "âœ¨ Model Translation:\n",
      "   The most eminent among the Brahmin priests, I desire to know of you the meaning of this verse, which refers to the sacred text in which the Brahmins have been teaching the people the knowledge of the Vedas.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 5/10: Mahabharata - Shanti Parva\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“œ Sanskrit:\n",
      "   à¤…à¤¹à¤¿à¤‚à¤¸à¤¾ à¤ªà¤°à¤®à¥‹ à¤§à¤°à¥à¤®à¤ƒ à¤§à¤°à¥à¤® à¤¹à¤¿à¤‚à¤¸à¤¾ à¤¤à¤¥à¥ˆà¤µ à¤šà¥¤ à¤¤à¤¸à¥à¤®à¤¾à¤¤à¥ à¤§à¤°à¥à¤®à¤ƒ à¤ªà¤°à¥‹ à¤°à¤¾à¤œà¤¨à¥ à¤¸à¤°à¥à¤µà¥‡à¤·à¤¾à¤®à¤¿à¤µ à¤µà¥ˆ à¤®à¤¤à¤ƒà¥¥\n",
      "\n",
      "ğŸ¯ Expected Context:\n",
      "   On non-violence and righteous violence\n",
      "\n",
      "ğŸ”„ Translating...\n",
      "\n",
      "âœ¨ Model Translation:\n",
      "   The MahÄbhÄrata is dear to VÄyurveda.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 6/10: Ramayana - Bala Kanda\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“œ Sanskrit:\n",
      "   à¤µà¤¿à¤¶à¥à¤µà¤¾à¤®à¤¿à¤¤à¥à¤°à¤µà¤šà¤ƒ à¤¶à¥à¤°à¥à¤¤à¥à¤µà¤¾ à¤°à¤¾à¤˜à¤µà¤ƒ à¤¸à¤¹à¤²à¤•à¥à¤·à¥à¤®à¤£à¤ƒà¥¤ à¤µà¤¿à¤¸à¥à¤®à¤¯à¤‚ à¤ªà¤°à¤®à¤‚ à¤—à¤¤à¥à¤µà¤¾ à¤µà¤¿à¤¶à¥à¤µà¤¾à¤®à¤¿à¤¤à¥à¤°à¤®à¤¥à¤¾à¤¬à¥à¤°à¤µà¥€à¤¤à¥à¥¥\n",
      "\n",
      "ğŸ¯ Expected Context:\n",
      "   Rama and Lakshmana listening to Vishwamitra\n",
      "\n",
      "ğŸ”„ Translating...\n",
      "\n",
      "âœ¨ Model Translation:\n",
      "   In respect of the son of the king, who is the eldest of the sons, the Queen said, \"I have heard that you are a warrior, and that you have been the conqueror of the country, and you are in the possession of the most powerful and richest of all the kingdoms.\"\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 7/10: Ramayana - Ayodhya Kanda\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“œ Sanskrit:\n",
      "   à¤°à¤¾à¤®à¥‹ à¤µà¤¿à¤—à¥à¤°à¤¹à¤µà¤¾à¤¨à¥ à¤§à¤°à¥à¤®à¤ƒ à¤¸à¤¾à¤§à¥à¤ƒ à¤¸à¤¤à¥à¤¯à¤ªà¤°à¤¾à¤•à¥à¤°à¤®à¤ƒà¥¤ à¤°à¤¾à¤œà¤¾ à¤¸à¤°à¥à¤µà¤¸à¥à¤¯ à¤²à¥‹à¤•à¤¸à¥à¤¯ à¤¦à¥‡à¤µà¤¾à¤¨à¤¾à¤®à¤¿à¤µ à¤µà¤¾à¤¸à¤µà¤ƒà¥¥\n",
      "\n",
      "ğŸ¯ Expected Context:\n",
      "   Description of Rama's virtues\n",
      "\n",
      "ğŸ”„ Translating...\n",
      "\n",
      "âœ¨ Model Translation:\n",
      "   RÄma would give the right measure to the RÄká¹£asas, and the Goddess would be happy with RÄma's desire.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 8/10: Ramayana - Ayodhya Kanda\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“œ Sanskrit:\n",
      "   à¤¨ à¤œà¤¾à¤¤à¥ à¤•à¤¾à¤®à¤¾à¤¨à¥à¤¨ à¤­à¤¯à¤¾à¤¨à¥à¤¨ à¤²à¥‹à¤­à¤¾à¤¦à¥ à¤§à¤°à¥à¤®à¤‚ à¤¤à¥à¤¯à¤œà¥‡à¤¯à¤‚ à¤ªà¥à¤°à¤¾à¤£à¥ˆà¤°à¤ªà¤¿ à¤—à¥à¤°à¥€à¤¯à¤¸à¤®à¥à¥¤ à¤¨ à¤œà¥€à¤µà¤¿à¤¤à¤¸à¥à¤¯à¤¾à¤•à¤¾à¤™à¥à¤•à¥à¤·à¤¾ à¤®à¥‡ à¤¨ à¤š à¤°à¤¾à¤œà¥à¤¯à¤¸à¥à¤¯ à¤•à¤°à¥à¤¹à¤¿à¤šà¤¿à¤¤à¥à¥¥\n",
      "\n",
      "ğŸ¯ Expected Context:\n",
      "   Rama's commitment to dharma\n",
      "\n",
      "ğŸ”„ Translating...\n",
      "\n",
      "âœ¨ Model Translation:\n",
      "   I had been saying that the gods had done nothing and had not pleased me, and that they had no knowledge of me, and that they had not put me in a state of affliction.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 9/10: Ramayana\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“œ Sanskrit:\n",
      "   à¤®à¤¾à¤¤à¥ƒà¤¦à¥‡à¤µà¥‹ à¤­à¤µ à¤ªà¤¿à¤¤à¥ƒà¤¦à¥‡à¤µà¥‹ à¤­à¤µ à¤†à¤šà¤¾à¤°à¥à¤¯à¤¦à¥‡à¤µà¥‹ à¤­à¤µ à¤…à¤¤à¤¿à¤¥à¤¿à¤¦à¥‡à¤µà¥‹ à¤­à¤µà¥¤\n",
      "\n",
      "ğŸ¯ Expected Context:\n",
      "   Respect for elders and guests\n",
      "\n",
      "ğŸ”„ Translating...\n",
      "\n",
      "âœ¨ Model Translation:\n",
      "   MÄtrÄ«dÌ§evo bhava pÄ«trÄ«dÌ§evo bhava achariya'devo bhava atita'devo bhava.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 10/10: Ramayana - Aranya Kanda\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“œ Sanskrit:\n",
      "   à¤¸à¥€à¤¤à¤¾ à¤®à¤® à¤ªà¥à¤°à¤¾à¤£à¥‡à¤­à¥à¤¯à¥‹à¤½à¤ªà¤¿ à¤—à¤°à¥€à¤¯à¤¸à¥€à¥¤ à¤œà¤¾à¤¨à¤•à¥€à¤®à¤¨à¥ƒà¤¤à¤¾à¤®à¤•à¥ƒà¤¤à¥à¤µà¤¾ à¤¨ à¤¶à¤•à¥à¤¤à¤ƒ à¤¸à¥à¤–à¤®à¤¾à¤ªà¥à¤¤à¥à¤®à¥à¥¥\n",
      "\n",
      "ğŸ¯ Expected Context:\n",
      "   Rama's love for Sita\n",
      "\n",
      "ğŸ”„ Translating...\n",
      "\n",
      "âœ¨ Model Translation:\n",
      "   I have been given full power to give up this world, and then I will become a god.\n",
      "\n",
      "====================================================================================================\n",
      "âœ… All translations completed!\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "SUMMARY OF TRANSLATIONS\n",
      "====================================================================================================\n",
      "\n",
      "1. Mahabharata - Bhagavad Gita 1.1\n",
      "   Sanskrit: à¤§à¥ƒà¤¤à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤‰à¤µà¤¾à¤šà¥¤ à¤§à¤°à¥à¤®à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‡ à¤•à¥à¤°à¥à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‡ à¤¸à¤®à¤µà¥‡à¤¤à¤¾ à¤¯à¥à¤¯à¥à¤¤à¥à¤¸à¤µà¤ƒà¥¤...\n",
      "   Generated: The DhrÌ„taÅ›á¹­rÄ are the two parts that are situated in the earth. They are the two parts that are sit...\n",
      "\n",
      "\n",
      "2. Mahabharata - Bhagavad Gita 4.7\n",
      "   Sanskrit: à¤¯à¤¦à¤¾ à¤¯à¤¦à¤¾ à¤¹à¤¿ à¤§à¤°à¥à¤®à¤¸à¥à¤¯ à¤—à¥à¤²à¤¾à¤¨à¤¿à¤°à¥à¤­à¤µà¤¤à¤¿ à¤­à¤¾à¤°à¤¤à¥¤ à¤…à¤­à¥à¤¯à¥à¤¤à¥à¤¥à¤¾à¤¨à¤®à¤§à¤°à¥à¤®à¤¸à¥à¤¯ à¤¤à¤¦à¤¾...\n",
      "   Generated: I have recited to you the words of the Dharma-sage, and those words are the words of the Dharma-sage...\n",
      "\n",
      "\n",
      "3. Mahabharata - Bhagavad Gita 2.47\n",
      "   Sanskrit: à¤•à¤°à¥à¤®à¤£à¥à¤¯à¥‡à¤µà¤¾à¤§à¤¿à¤•à¤¾à¤°à¤¸à¥à¤¤à¥‡ à¤®à¤¾ à¤«à¤²à¥‡à¤·à¥ à¤•à¤¦à¤¾à¤šà¤¨à¥¤ à¤®à¤¾ à¤•à¤°à¥à¤®à¤«à¤²à¤¹à¥‡à¤¤à¥à¤°à¥à¤­à¥‚à¤°à¥à¤®à¤¾ à¤¤à¥‡...\n",
      "   Generated: With the aid of a great brahmana, they [the people] learned all the Vedas....\n",
      "\n",
      "\n",
      "4. Mahabharata - Udyoga Parva\n",
      "   Sanskrit: à¤¸à¤¤à¥à¤¯à¤‚ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤ªà¥à¤°à¤¿à¤¯à¤‚ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤¨ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤¸à¤¤à¥à¤¯à¤®à¤ªà¥à¤°à¤¿à¤¯à¤®à¥à¥¤ à¤ªà¥à¤°à¤¿...\n",
      "   Generated: The most eminent among the Brahmin priests, I desire to know of you the meaning of this verse, which...\n",
      "\n",
      "\n",
      "5. Mahabharata - Shanti Parva\n",
      "   Sanskrit: à¤…à¤¹à¤¿à¤‚à¤¸à¤¾ à¤ªà¤°à¤®à¥‹ à¤§à¤°à¥à¤®à¤ƒ à¤§à¤°à¥à¤® à¤¹à¤¿à¤‚à¤¸à¤¾ à¤¤à¤¥à¥ˆà¤µ à¤šà¥¤ à¤¤à¤¸à¥à¤®à¤¾à¤¤à¥ à¤§à¤°à¥à¤®à¤ƒ à¤ªà¤°à¥‹ à¤°à¤¾à¤œà¤¨à¥...\n",
      "   Generated: The MahÄbhÄrata is dear to VÄyurveda....\n",
      "\n",
      "\n",
      "6. Ramayana - Bala Kanda\n",
      "   Sanskrit: à¤µà¤¿à¤¶à¥à¤µà¤¾à¤®à¤¿à¤¤à¥à¤°à¤µà¤šà¤ƒ à¤¶à¥à¤°à¥à¤¤à¥à¤µà¤¾ à¤°à¤¾à¤˜à¤µà¤ƒ à¤¸à¤¹à¤²à¤•à¥à¤·à¥à¤®à¤£à¤ƒà¥¤ à¤µà¤¿à¤¸à¥à¤®à¤¯à¤‚ à¤ªà¤°à¤®à¤‚ à¤—à¤¤à¥à¤µà¤¾...\n",
      "   Generated: In respect of the son of the king, who is the eldest of the sons, the Queen said, \"I have heard that...\n",
      "\n",
      "\n",
      "7. Ramayana - Ayodhya Kanda\n",
      "   Sanskrit: à¤°à¤¾à¤®à¥‹ à¤µà¤¿à¤—à¥à¤°à¤¹à¤µà¤¾à¤¨à¥ à¤§à¤°à¥à¤®à¤ƒ à¤¸à¤¾à¤§à¥à¤ƒ à¤¸à¤¤à¥à¤¯à¤ªà¤°à¤¾à¤•à¥à¤°à¤®à¤ƒà¥¤ à¤°à¤¾à¤œà¤¾ à¤¸à¤°à¥à¤µà¤¸à¥à¤¯ à¤²à¥‹à¤•à¤¸à¥...\n",
      "   Generated: RÄma would give the right measure to the RÄká¹£asas, and the Goddess would be happy with RÄma's desire...\n",
      "\n",
      "\n",
      "8. Ramayana - Ayodhya Kanda\n",
      "   Sanskrit: à¤¨ à¤œà¤¾à¤¤à¥ à¤•à¤¾à¤®à¤¾à¤¨à¥à¤¨ à¤­à¤¯à¤¾à¤¨à¥à¤¨ à¤²à¥‹à¤­à¤¾à¤¦à¥ à¤§à¤°à¥à¤®à¤‚ à¤¤à¥à¤¯à¤œà¥‡à¤¯à¤‚ à¤ªà¥à¤°à¤¾à¤£à¥ˆà¤°à¤ªà¤¿ à¤—à¥à¤°à¥€à¤¯à¤¸à¤®...\n",
      "   Generated: I had been saying that the gods had done nothing and had not pleased me, and that they had no knowle...\n",
      "\n",
      "\n",
      "9. Ramayana\n",
      "   Sanskrit: à¤®à¤¾à¤¤à¥ƒà¤¦à¥‡à¤µà¥‹ à¤­à¤µ à¤ªà¤¿à¤¤à¥ƒà¤¦à¥‡à¤µà¥‹ à¤­à¤µ à¤†à¤šà¤¾à¤°à¥à¤¯à¤¦à¥‡à¤µà¥‹ à¤­à¤µ à¤…à¤¤à¤¿à¤¥à¤¿à¤¦à¥‡à¤µà¥‹ à¤­à¤µà¥¤...\n",
      "   Generated: MÄtrÄ«dÌ§evo bhava pÄ«trÄ«dÌ§evo bhava achariya'devo bhava atita'devo bhava....\n",
      "\n",
      "\n",
      "10. Ramayana - Aranya Kanda\n",
      "   Sanskrit: à¤¸à¥€à¤¤à¤¾ à¤®à¤® à¤ªà¥à¤°à¤¾à¤£à¥‡à¤­à¥à¤¯à¥‹à¤½à¤ªà¤¿ à¤—à¤°à¥€à¤¯à¤¸à¥€à¥¤ à¤œà¤¾à¤¨à¤•à¥€à¤®à¤¨à¥ƒà¤¤à¤¾à¤®à¤•à¥ƒà¤¤à¥à¤µà¤¾ à¤¨ à¤¶à¤•à¥à¤¤à¤ƒ à¤¸à¥à¤–à¤®...\n",
      "   Generated: I have been given full power to give up this world, and then I will become a god....\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "TESTING DIFFERENT GENERATION STRATEGIES\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“œ Test Sanskrit: à¤§à¤°à¥à¤®à¥‹ à¤°à¤•à¥à¤·à¤¤à¤¿ à¤°à¤•à¥à¤·à¤¿à¤¤à¤ƒ\n",
      "ğŸ¯ Expected: Dharma protects those who protect it\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Strategy: Low Temperature (Conservative)\n",
      "  Temperature: 0.3, Beams: 1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Translation: The Lord of Raksasas, Raksasas.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Strategy: Medium Temperature (Balanced)\n",
      "  Temperature: 0.7, Beams: 1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Translation: He is the one who is born of Ká¹›á¹£á¹‡a, whom MÄdhava is the son of.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Strategy: High Temperature (Creative)\n",
      "  Temperature: 1.0, Beams: 1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Translation: My dear SÅ«rya, if it seems to you, after hearing my exhortations, that the Sankalpa which is now being conducted against me is not suitable, then what remains, O mighty one, you should not have undertaken it.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Strategy: Beam Search (Deterministic)\n",
      "  Temperature: 0.7, Beams: 3\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Translation: He who is the lord of the Raksasas.\n",
      "\n",
      "The Sanskrit phrase \"à¤§à¤°à¥à¤®à¥‹ à¤°à¤•à¥à¤·à¤¤à¤¿ à¤°à¤•à¥à¤·à¤¿à¤¤à¤ƒ\" translates to English as \"He who is the lord of the Raksasas.\" In this context, \"à¤§à¤°à¥à¤®à¥‹\" means \"he,\" \"à¤°à¤•à¥à¤·à¤¤à¤¿\" means \"lord,\" and \"à¤°à¤•à¥à¤·à¤¿à¤¤à¤ƒ\" means \"of the Raksasas.\"\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Testing Trained Qwen-Itihasa LoRA Model\n",
    "# Test on multiple Sanskrit samples from Mahabharata & Ramayana\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# %%\n",
    "# Load the Trained Model\n",
    "# Paths\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-0.5B\"\n",
    "LORA_MODEL = \"./qwen-itihasa-lora\"\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, LORA_MODEL)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL)\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\\n\")\n",
    "\n",
    "# %%\n",
    "# Test samples from Mahabharata and Ramayana\n",
    "\n",
    "# Sample Sanskrit texts for testing - Mahabharata & Ramayana\n",
    "test_samples = [\n",
    "    # MAHABHARATA\n",
    "    {\n",
    "        \"text\": \"à¤§à¥ƒà¤¤à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤‰à¤µà¤¾à¤šà¥¤ à¤§à¤°à¥à¤®à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‡ à¤•à¥à¤°à¥à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‡ à¤¸à¤®à¤µà¥‡à¤¤à¤¾ à¤¯à¥à¤¯à¥à¤¤à¥à¤¸à¤µà¤ƒà¥¤ à¤®à¤¾à¤®à¤•à¤¾à¤ƒ à¤ªà¤¾à¤£à¥à¤¡à¤µà¤¾à¤¶à¥à¤šà¥ˆà¤µ à¤•à¤¿à¤®à¤•à¥à¤°à¥à¤µà¤¤ à¤¸à¤à¥à¤œà¤¯à¥¥\",\n",
    "        \"source\": \"Mahabharata - Bhagavad Gita 1.1\",\n",
    "        \"context\": \"Dhritarashtra's opening question about the battle\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"à¤¯à¤¦à¤¾ à¤¯à¤¦à¤¾ à¤¹à¤¿ à¤§à¤°à¥à¤®à¤¸à¥à¤¯ à¤—à¥à¤²à¤¾à¤¨à¤¿à¤°à¥à¤­à¤µà¤¤à¤¿ à¤­à¤¾à¤°à¤¤à¥¤ à¤…à¤­à¥à¤¯à¥à¤¤à¥à¤¥à¤¾à¤¨à¤®à¤§à¤°à¥à¤®à¤¸à¥à¤¯ à¤¤à¤¦à¤¾à¤¤à¥à¤®à¤¾à¤¨à¤‚ à¤¸à¥ƒà¤œà¤¾à¤®à¥à¤¯à¤¹à¤®à¥à¥¥\",\n",
    "        \"source\": \"Mahabharata - Bhagavad Gita 4.7\",\n",
    "        \"context\": \"Krishna on divine incarnation\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"à¤•à¤°à¥à¤®à¤£à¥à¤¯à¥‡à¤µà¤¾à¤§à¤¿à¤•à¤¾à¤°à¤¸à¥à¤¤à¥‡ à¤®à¤¾ à¤«à¤²à¥‡à¤·à¥ à¤•à¤¦à¤¾à¤šà¤¨à¥¤ à¤®à¤¾ à¤•à¤°à¥à¤®à¤«à¤²à¤¹à¥‡à¤¤à¥à¤°à¥à¤­à¥‚à¤°à¥à¤®à¤¾ à¤¤à¥‡ à¤¸à¤™à¥à¤—à¥‹à¤½à¤¸à¥à¤¤à¥à¤µà¤•à¤°à¥à¤®à¤£à¤¿à¥¥\",\n",
    "        \"source\": \"Mahabharata - Bhagavad Gita 2.47\",\n",
    "        \"context\": \"Famous verse on karma yoga\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"à¤¸à¤¤à¥à¤¯à¤‚ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤ªà¥à¤°à¤¿à¤¯à¤‚ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤¨ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤¸à¤¤à¥à¤¯à¤®à¤ªà¥à¤°à¤¿à¤¯à¤®à¥à¥¤ à¤ªà¥à¤°à¤¿à¤¯à¤‚ à¤š à¤¨à¤¾à¤¨à¥ƒà¤¤à¤‚ à¤¬à¥à¤°à¥‚à¤¯à¤¾à¤¤à¥ à¤à¤· à¤§à¤°à¥à¤®à¤ƒ à¤¸à¤¨à¤¾à¤¤à¤¨à¤ƒà¥¥\",\n",
    "        \"source\": \"Mahabharata - Udyoga Parva\",\n",
    "        \"context\": \"Principles of truthful speech\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"à¤…à¤¹à¤¿à¤‚à¤¸à¤¾ à¤ªà¤°à¤®à¥‹ à¤§à¤°à¥à¤®à¤ƒ à¤§à¤°à¥à¤® à¤¹à¤¿à¤‚à¤¸à¤¾ à¤¤à¤¥à¥ˆà¤µ à¤šà¥¤ à¤¤à¤¸à¥à¤®à¤¾à¤¤à¥ à¤§à¤°à¥à¤®à¤ƒ à¤ªà¤°à¥‹ à¤°à¤¾à¤œà¤¨à¥ à¤¸à¤°à¥à¤µà¥‡à¤·à¤¾à¤®à¤¿à¤µ à¤µà¥ˆ à¤®à¤¤à¤ƒà¥¥\",\n",
    "        \"source\": \"Mahabharata - Shanti Parva\",\n",
    "        \"context\": \"On non-violence and righteous violence\"\n",
    "    },\n",
    "    \n",
    "    # RAMAYANA\n",
    "    {\n",
    "        \"text\": \"à¤µà¤¿à¤¶à¥à¤µà¤¾à¤®à¤¿à¤¤à¥à¤°à¤µà¤šà¤ƒ à¤¶à¥à¤°à¥à¤¤à¥à¤µà¤¾ à¤°à¤¾à¤˜à¤µà¤ƒ à¤¸à¤¹à¤²à¤•à¥à¤·à¥à¤®à¤£à¤ƒà¥¤ à¤µà¤¿à¤¸à¥à¤®à¤¯à¤‚ à¤ªà¤°à¤®à¤‚ à¤—à¤¤à¥à¤µà¤¾ à¤µà¤¿à¤¶à¥à¤µà¤¾à¤®à¤¿à¤¤à¥à¤°à¤®à¤¥à¤¾à¤¬à¥à¤°à¤µà¥€à¤¤à¥à¥¥\",\n",
    "        \"source\": \"Ramayana - Bala Kanda\",\n",
    "        \"context\": \"Rama and Lakshmana listening to Vishwamitra\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"à¤°à¤¾à¤®à¥‹ à¤µà¤¿à¤—à¥à¤°à¤¹à¤µà¤¾à¤¨à¥ à¤§à¤°à¥à¤®à¤ƒ à¤¸à¤¾à¤§à¥à¤ƒ à¤¸à¤¤à¥à¤¯à¤ªà¤°à¤¾à¤•à¥à¤°à¤®à¤ƒà¥¤ à¤°à¤¾à¤œà¤¾ à¤¸à¤°à¥à¤µà¤¸à¥à¤¯ à¤²à¥‹à¤•à¤¸à¥à¤¯ à¤¦à¥‡à¤µà¤¾à¤¨à¤¾à¤®à¤¿à¤µ à¤µà¤¾à¤¸à¤µà¤ƒà¥¥\",\n",
    "        \"source\": \"Ramayana - Ayodhya Kanda\",\n",
    "        \"context\": \"Description of Rama's virtues\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"à¤¨ à¤œà¤¾à¤¤à¥ à¤•à¤¾à¤®à¤¾à¤¨à¥à¤¨ à¤­à¤¯à¤¾à¤¨à¥à¤¨ à¤²à¥‹à¤­à¤¾à¤¦à¥ à¤§à¤°à¥à¤®à¤‚ à¤¤à¥à¤¯à¤œà¥‡à¤¯à¤‚ à¤ªà¥à¤°à¤¾à¤£à¥ˆà¤°à¤ªà¤¿ à¤—à¥à¤°à¥€à¤¯à¤¸à¤®à¥à¥¤ à¤¨ à¤œà¥€à¤µà¤¿à¤¤à¤¸à¥à¤¯à¤¾à¤•à¤¾à¤™à¥à¤•à¥à¤·à¤¾ à¤®à¥‡ à¤¨ à¤š à¤°à¤¾à¤œà¥à¤¯à¤¸à¥à¤¯ à¤•à¤°à¥à¤¹à¤¿à¤šà¤¿à¤¤à¥à¥¥\",\n",
    "        \"source\": \"Ramayana - Ayodhya Kanda\",\n",
    "        \"context\": \"Rama's commitment to dharma\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"à¤®à¤¾à¤¤à¥ƒà¤¦à¥‡à¤µà¥‹ à¤­à¤µ à¤ªà¤¿à¤¤à¥ƒà¤¦à¥‡à¤µà¥‹ à¤­à¤µ à¤†à¤šà¤¾à¤°à¥à¤¯à¤¦à¥‡à¤µà¥‹ à¤­à¤µ à¤…à¤¤à¤¿à¤¥à¤¿à¤¦à¥‡à¤µà¥‹ à¤­à¤µà¥¤\",\n",
    "        \"source\": \"Ramayana\",\n",
    "        \"context\": \"Respect for elders and guests\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"à¤¸à¥€à¤¤à¤¾ à¤®à¤® à¤ªà¥à¤°à¤¾à¤£à¥‡à¤­à¥à¤¯à¥‹à¤½à¤ªà¤¿ à¤—à¤°à¥€à¤¯à¤¸à¥€à¥¤ à¤œà¤¾à¤¨à¤•à¥€à¤®à¤¨à¥ƒà¤¤à¤¾à¤®à¤•à¥ƒà¤¤à¥à¤µà¤¾ à¤¨ à¤¶à¤•à¥à¤¤à¤ƒ à¤¸à¥à¤–à¤®à¤¾à¤ªà¥à¤¤à¥à¤®à¥à¥¥\",\n",
    "        \"source\": \"Ramayana - Aranya Kanda\",\n",
    "        \"context\": \"Rama's love for Sita\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(test_samples)} test samples\")\n",
    "\n",
    "# %%\n",
    "# Translation Function\n",
    "def translate_sanskrit(text, max_new_tokens=128, temperature=0.7, num_beams=1):\n",
    "    \"\"\"\n",
    "    Translate Sanskrit text to English using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        text: Sanskrit text to translate\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "        temperature: Sampling temperature (lower = more conservative)\n",
    "        num_beams: Number of beams for beam search (1 = greedy/sampling)\n",
    "    \"\"\"\n",
    "    # Create prompt in the same format as training\n",
    "    prompt = f\"Translate this Sanskrit shloka to English:\\n{text}\\n\\nTranslation:\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,  # Changed from max_length\n",
    "            num_return_sequences=1,\n",
    "            temperature=temperature,\n",
    "            do_sample=True if num_beams == 1 else False,\n",
    "            num_beams=num_beams,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the translation (remove the prompt)\n",
    "    if \"Translation:\" in generated_text:\n",
    "        translation = generated_text.split(\"Translation:\")[-1].strip()\n",
    "    else:\n",
    "        translation = generated_text\n",
    "    \n",
    "    return translation\n",
    "\n",
    "print(\"âœ… Translation function ready\")\n",
    "\n",
    "# %%\n",
    "# Run Tests on All Samples\n",
    "print(\"=\"*100)\n",
    "print(\"SANSKRIT TO ENGLISH TRANSLATION TESTS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    print(f\"\\n{'â”€'*100}\")\n",
    "    print(f\"Test {i}/{len(test_samples)}: {sample['source']}\")\n",
    "    print(f\"{'â”€'*100}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“œ Sanskrit:\")\n",
    "    print(f\"   {sample['text']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Expected Context:\")\n",
    "    print(f\"   {sample['context']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Translating...\")\n",
    "    translation = translate_sanskrit(sample['text'])\n",
    "    \n",
    "    print(f\"\\nâœ¨ Model Translation:\")\n",
    "    print(f\"   {translation}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"source\": sample['source'],\n",
    "        \"sanskrit\": sample['text'],\n",
    "        \"context\": sample['context'],\n",
    "        \"generated\": translation\n",
    "    })\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"âœ… All translations completed!\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# %%\n",
    "# Summary Results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY OF TRANSLATIONS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {result['source']}\")\n",
    "    print(f\"   Sanskrit: {result['sanskrit'][:60]}...\")\n",
    "    print(f\"   Generated: {result['generated'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "# %%\n",
    "# Test with Different Generation Strategies\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TESTING DIFFERENT GENERATION STRATEGIES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "test_text = \"à¤§à¤°à¥à¤®à¥‹ à¤°à¤•à¥à¤·à¤¤à¤¿ à¤°à¤•à¥à¤·à¤¿à¤¤à¤ƒ\"\n",
    "\n",
    "strategies = [\n",
    "    {\"name\": \"Low Temperature (Conservative)\", \"temperature\": 0.3, \"num_beams\": 1},\n",
    "    {\"name\": \"Medium Temperature (Balanced)\", \"temperature\": 0.7, \"num_beams\": 1},\n",
    "    {\"name\": \"High Temperature (Creative)\", \"temperature\": 1.0, \"num_beams\": 1},\n",
    "    {\"name\": \"Beam Search (Deterministic)\", \"temperature\": 0.7, \"num_beams\": 3},\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ“œ Test Sanskrit: {test_text}\")\n",
    "print(f\"ğŸ¯ Expected: Dharma protects those who protect it\\n\")\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\n{'â”€'*100}\")\n",
    "    print(f\"Strategy: {strategy['name']}\")\n",
    "    print(f\"  Temperature: {strategy['temperature']}, Beams: {strategy['num_beams']}\")\n",
    "    print(f\"{'â”€'*100}\")\n",
    "    \n",
    "    translation = translate_sanskrit(\n",
    "        test_text,\n",
    "        temperature=strategy['temperature'],\n",
    "        num_beams=strategy['num_beams']\n",
    "    )\n",
    "    \n",
    "    print(f\"Translation: {translation}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "243fdd99-e90c-4f9e-9316-015edaa52a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ã Â¤Â°', 'Ã Â¤Â¾Ã Â¤', 'Â®', 'Ã Â¤Â¾Ã Â¤', 'Â¯', 'Ã Â¤Â£']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)  # replace with your model\n",
    "sample_text = \"à¤°à¤¾à¤®à¤¾à¤¯à¤£\"\n",
    "print(tokenizer.tokenize(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90635efb-c7a0-4e85-9642-f143e2054991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen-env",
   "language": "python",
   "name": "qwen-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
